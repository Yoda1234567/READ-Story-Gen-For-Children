# Authors: Code generated by ChatGPT online and comments done by Daniel Holgate
# Date: 16/09/2025
# Description: Code for prompting LLaMA to go through the base story dataset and extract the transitions between each pair of paragraphs
# to be used later during auto-regressive stage.

import re
from datetime import datetime
import os
import subprocess

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
# Must change them for your setup.
LLAMA_CLI_PATH = ""
MODEL_PATH = ""

MAX_CONTEXT_TOKENS = 8192 # Specifies the total number of tokens that the model can process at once.
# Specifies the limit of tokens that can be generated by the model in response to a prompt. Much smaller here because want the output
# to be as short as possible
MAX_GENERATION_TOKENS_PER_BATCH = 50 

# Files for input and output. Need to be adjusted for the files you are using. 
# The input is the source stories dataset you want to use and the output can be what you want.
INPUT_FILE_PATH = ""
OUTPUT_FILE_PATH = ""

# Method for estimating the number of tokens in a text. This will vary in different situations so just an estimate based on the 
# common conversion between words and tokens.
def simple_token_estimate(text):
    return len(text.split()) * 1.3 

# The main function for processing and extracting paragraph transitions in stories. 
# Takes in an input file, performs what is requested in the prompt and outputs the story text split into paragraph and with
# paragraph transitions.
def extract_paragraph_transitions_per_story(
    input_file: str,
    output_file: str,
    llama_cli_path: str,
    model_path: str,
    max_context_tokens: int,
    max_generation_tokens_per_batch: int
):

    # Attempts to read in the entire input file.
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            full_text = f.read()
    except FileNotFoundError:
        print(f"Error: Input file '{input_file}' not found.")
        return

    # Splits read text into separate stories (are separated by double empty line in the datasets)
    stories_raw = re.split(r'\n\s*\n', full_text.strip(), flags=re.MULTILINE)
    stories = [story.strip() for story in stories_raw if story.strip()]
    print(f"Found {len(stories)} stories.")

    # Provides the prompt to the model for instructing it in how to extract paragraph transitions.
    instruction_prompt_template = """
    I want to determine the generalised narrative transitions between paragraphs in children's stories.

    Analyze the transition between the two paragraphs below and describe the type of 
    narrative shift or connection that occurs between them.

    Instructions for your output:
    - Output just one generalized narrative transition type type (e.g., "Character motivation revealed", "Tension rises", 
    "New setting introduced") giving only the transition types and nothing else.
    - Focus on abstract, transferable transition types that could apply to many stories.
    - If there is no meaningful transition, respond with "No transition".

    [Paragraph 1]
    {paragraph_1}

    [Paragraph 2]
    {paragraph_2}
    """
    # Calculates tokens in the prompt.
    overhead_prompt_tokens = simple_token_estimate(
        instruction_prompt_template.format(paragraph_1="", paragraph_2="")
    )
    # Calculates number of tokens available for passing input stories to the model.
    MAX_TOKENS_FOR_PARAGRAPHS = max_context_tokens - overhead_prompt_tokens - max_generation_tokens_per_batch

    with open(output_file, "w", encoding="utf-8") as outfile:
        outfile.write("--- Start of Paragraph-to-Transition Corpus ---\n\n")

        # Loops through stories one at a time.
        for story_idx, story in enumerate(stories, 1):
            # Splits each story into sentences so that can create three sentence paragraphs.
            sentences = re.split(r'(?<=[.!?])\s+', story)
            sentences = [s.strip() for s in sentences if s.strip()]

            # Creates three sentence paragraphs by joining the individual sentences.
            paragraphs = [
                " ".join(sentences[i:i + 3])
                for i in range(0, len(sentences), 3)
            ]

            # Only uses stories with at least two paragraphs i.e. at least 6 sentences. Cannot have any paragraph transitions otherwise.
            if len(paragraphs) < 2:
                print(f"Skipping story {story_idx}: not enough paragraphs.")
                continue

            # Writes story id to output file.
            outfile.write(f"\n--- Story {story_idx} ---\n\n")

            # Writes the story in its separate paragraphs to the output file.
            for p_idx, para in enumerate(paragraphs, 1):
                outfile.write(f"[Paragraph {p_idx}]\n{para}\n\n")

            outfile.write("--- Paragraph Transitions ---\n\n")

            # For each pair of paragraphs, passes them to the model with the prompt to process and determine the paragraph transition.
            for i in range(len(paragraphs) - 1):
                para1 = paragraphs[i]
                para2 = paragraphs[i + 1]
                token_estimate = simple_token_estimate(para1 + para2)
                if token_estimate > MAX_TOKENS_FOR_PARAGRAPHS:
                    outfile.write(f"[Transition {i + 1}] Skipped: Paragraphs too long for context window.\n\n")
                    continue

                transition = process_paragraph_pair(
                    para1,
                    para2,
                    llama_cli_path,
                    model_path,
                    max_context_tokens,
                    max_generation_tokens_per_batch,
                    instruction_prompt_template
                )
                
                # Writes the extracted transition to the output file.
                outfile.write(f"[Transition {i + 1}]\n{transition if transition else 'Error or no output'}\n\n")


# Method for processing a pair of paragraphs to extract the transition from the first to the second paragraph.
def process_paragraph_pair(
    para1: str,
    para2: str,
    llama_cli_path: str,
    model_path: str,
    max_context_tokens: int,
    max_generation_tokens_per_batch: int,
    instruction_prompt_template: str
) -> str:
    prompt = instruction_prompt_template.format(paragraph_1=para1, paragraph_2=para2)

    # Specifies the format of the prompt so that the model is passed things in an order it understands and knows what to process
    # and what its ouput is.
    llama_cli_prompt_input_string = (
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
        f"{prompt}"
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    )

    # Specifies what variables need to be passed to what part of the model.
    command = [
        llama_cli_path,
        "-m", model_path,
        "-c", str(max_context_tokens),
        "-n", str(max_generation_tokens_per_batch),
        # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
        # probable next token. 
        "--temp", "0.7",
        # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
        # words.
        "--top-p", "0.9",
        # Random number generator seed. If same one with same prompt and parameters should always get the same out.
        "--seed", "1",
        "--prompt", llama_cli_prompt_input_string
    ]

    try:
        # Runs the specified command for the model and returns what the model outputs to the standard output.
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=True,
            encoding="utf-8"
        )
        raw_output = result.stdout

        assistant_marker = "assistant\n\n"
        # Searches for assistant marker so knows where output of actual response begins.
        start_idx = raw_output.rfind(assistant_marker)

        # If there is an assitant marker, cuts out everything of the marker and before.
        if start_idx != -1:
            clean_output = raw_output[start_idx + len(assistant_marker):].strip()
        # Otherwise just uses the full output.
        else:
            clean_output = raw_output.strip()

        # Removes the beginning and end of text markers.
        clean_output = clean_output.replace("<|eot_id|>", "").replace("[end of text]", "").strip()

        for line in clean_output.splitlines():
            if line.strip():
                return line.strip()

        return ""

    # Catches processing and extraction errors.
    except subprocess.CalledProcessError as e:
        print(f"Subprocess error: {e}")
        return ""
    except Exception as e:
        print(f"Error during transition extraction: {e}")
        return ""

# Main method for calling all the above by calling extract_paragraph_transitions_per_story.
if __name__ == "__main__":
    extract_paragraph_transitions_per_story(
        input_file=INPUT_FILE_PATH,
        output_file=OUTPUT_FILE_PATH,
        llama_cli_path=LLAMA_CLI_PATH,
        model_path=MODEL_PATH,
        max_context_tokens=MAX_CONTEXT_TOKENS,
        max_generation_tokens_per_batch=MAX_GENERATION_TOKENS_PER_BATCH
    )
