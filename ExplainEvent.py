# Authors: Code generated by Google Gemini and comments done by Daniel Holgate
# Date: 16/09/2025
# Description: Code for prompting LLaMA3.2-3B-Instruct-Q4_K_M.gguf to extract generalisable story events from a story dataset.

import subprocess
import datetime
import traceback

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
LLAMA_CLI_PATH = ""
MODEL_PATH = ""

BATCH_SIZE = 4 # Number of events to attempt explaining at a time.
MAX_CONTEXT_TOKENS = 8192 # Specifies the total number of tokens that the model can process at once.
# Specifies the limit of tokens that can be generated by the model in response to a prompt. Could be fewer.
# Compared to MAX_GENERATION_TOKENS_PER_BATCH in Events.py says leave this number of tokens available in case I want to use them.
# This is better for where prompts may vary in length and want to ensure you don't get runtime errors.
GENERATION_BUFFER = 2048 

# Files for input and output. Need to be adjusted for the files you are using. 
# The input is the events file created by Events.py and the output can be whatever you want the raw output file to be called.
INPUT_FILE = ""
RAW_OUTPUT_FILE = ""

# Method for estimating the number of tokens in a text. This will vary in different situations so just an estimate based on the 
# common conversion between words and tokens.
def simple_token_estimate(text):
    # Often have each word being approximately 1.3 tokens.
    return len(text.split())*1.3

# Method for reading in the events file and creating a list from them.
def read_elements_from_file(path):
    with open(path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    elements = [
        line.strip().lstrip("- ").strip()
        for line in lines
        if line.strip().startswith("-")
    ]
    return elements

# Provides the prompt to the model for instructing it in how to explain the generalisable events so they can be easily
# passed to the model at the generation phase.
def build_simple_prompt(batch):
    instructions = (
        """
        I would like to write descriptions for how to include general events in children's stories for a Large Language Model.

        Below is a list of story elements provided as bullet points.\n
        For each bullet point, write 2â€“3 sentences describing how to include it in a general story.\n
        
        Output the final event description pair as:
            Event:
            Description:

        BEGIN ELEMENTS:\n
        """
    )

    # Collects all the events in the batch together.
    elements_text = ""
    for element in batch:
        elements_text += f"- {element}\n"

    # Adds the prompt on the front of the events.
    return instructions + elements_text + "\n"

# Method for processing a batch of stories using llama-cli.
def run_llama_cli_batch(batch, batch_index):
    prompt = build_simple_prompt(batch)

    # Specifies what variables need to be passed to what part of the model.
    command = [
        LLAMA_CLI_PATH,
        "-m", MODEL_PATH,
        "-c", str(MAX_CONTEXT_TOKENS),
        "-n", str(GENERATION_BUFFER),
        # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
        # probable next token.
        "--temp", "0.85",
        # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
        # words.
        "--top-p", "0.95",
        # Random number generator seed. If same one with same prompt and parameters should always get the same out.
        "--seed", "42",
        "--prompt", prompt
    ]

    try:
        # Runs the specified command for the model and returns what the model outputs to the standard output.
        result = subprocess.run(
            command,
            check=True,
            # Captures output and errors so can be processed rather than just outputting them to the terminal.
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            encoding="utf-8",
            # Replaces errors in generation with a specified character. Better for longer, less structured generation.
            errors="replace"
        )
        output = result.stdout.strip().replace('\r\n', '\n')
        output = output.replace("[end of text]", "").replace("[End of text]", "").strip()

        # Appends the raw output generated by the model. Initially tried to format it but model seemed to get confused, 
        # so now written to a file and the raw output is then formatted by another python script which works more consistently.
        with open(RAW_OUTPUT_FILE, "a", encoding="utf-8") as f:
            generated_start = output.find("Event:")
            if generated_start != -1:
                generated_only = output[generated_start:].strip()
            else:
                generated_only = output.strip()

            # Save only generated part
            with open(RAW_OUTPUT_FILE, "a", encoding="utf-8") as f:
                f.write(f"\n\n=== Batch {batch_index} ===\n\n")
                f.write(generated_only)

        return generated_only

    except Exception as e:
        print("\nUnexpected Python error while calling llama-cli:")
        traceback.print_exc()
        return ""

# Main function for calling all the above methods.
def main():
    story_elements = read_elements_from_file(INPUT_FILE)
    
    # Overwrites previous raw output file before starting on processing.
    with open(RAW_OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write(f"Raw LLaMA outputs - started at {datetime.datetime.now()}\n")

    # Runs the batch processing on each batch until all the events have been processed.
    for i in range(0, len(story_elements), BATCH_SIZE):
        batch = story_elements[i:i + BATCH_SIZE]
        run_llama_cli_batch(batch, batch_index=(i // BATCH_SIZE) + 1)

if __name__ == "__main__":
    main()
