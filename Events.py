# Authors: Code generated by Google Gemini and comments done by Daniel Holgate
# Date: 16/09/2025
# Description: Code for prompting LLaMA3.2-3B-Instruct-Q4_K_M.gguf to extract generalisable story events from a story dataset.

import re
from datetime import datetime
import os
import subprocess

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
# Must change them for your setup.
LLAMA_CLI_PATH = ""
MODEL_PATH = ""

MAX_CONTEXT_TOKENS = 8192 # Specifies the total number of tokens that the model can process at once.
MAX_GENERATION_TOKENS_PER_BATCH = 1000 # Specifies the limit of tokens that can be generated by the model in response to a prompt.

STORY_DELIMITER = "\n\n"

# Files for input and output. Need to be adjusted for the files you are using. 
# The input is the source stories dataset you want to use and the output can be what you want.
INPUT_FILE_PATH = ""
OUTPUT_FILE_PATH = "" 

# Method for estimating the number of tokens in a text. This will vary in different situations so just an estimate based on the 
# common conversion between words and tokens.
def simple_token_estimate(text):
    """Estimates tokens based on word count, for batching purposes."""
    # Often have each word being approximately 1.3 tokens.
    return len(text.split()) * 1.3

# The main function for processing and extracting generalisable story events. 
# Takes in an input file, performs what is requested in the prompt and outputs non-repeated, generalisable events to an output file.
def extract_narrative_event_types(
    input_file: str,
    output_file: str,
    llama_cli_path: str,
    model_path: str,
    story_delimiter: str,
    max_context_tokens: int,
    max_generation_tokens_per_batch: int):
    
    # print(f"Starting corpus-wide narrative event type extraction with llama.cpp CLI at {datetime.now()}")
    # print(f"Input file: {input_file}")
    # print(f"Output file: {output_file}")
    # print(f"llama-cli path: {llama_cli_path}")
    # print(f"Model path: {model_path}")
    # print(f"Max context tokens for model (-c): {max_context_tokens}")
    # print(f"Max generation tokens per batch (-n): {max_generation_tokens_per_batch}")

    # Attempts to read in the story dataset for processing.
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            full_text = f.read()
    except FileNotFoundError:
        print(f"Error: Input file '{input_file}' not found.")
        return
    except Exception as e:
        print(f"Error reading input file: {e}")
        return

    # Splits read text into separate stories (are separated by double empty line in the datasets)
    stories_raw = re.split(r'\n\s*\n', full_text.strip(), flags=re.MULTILINE)
    stories = [story.strip() for story in stories_raw if story.strip()]

    if not stories:
        print("No stories found in the input file. Check delimiter and content.")
        return

    print(f"Found {len(stories)} stories in the file.")

    all_extracted_event_types = []

    # Provides the prompt to the model for instructing it in how to extract the generalisable events. Provides examples and instructions.
    # Specifies age to try lead the model to events appropriate for young children's stories.
    instruction_prompt_template = """
    I want to make a list of distinct, highly generalisable narrative 
    event types or structural patterns from stories.
    
    Identify all distinct, highly generalisable narrative 
    event types or structural patterns present within this batch.
    These narrative event types should be abstract categories or 
    common story beats, not specific plot points from the narratives. 
    They should be phrased concisely, describing the core narrative function or pattern.

    Examples of Generalisable Narrative Event Types (use this style):
    - Character introduction
    - Plot twist
    - Challenge or obstacle

    Instructions for your output:
    - Provide a single, de-duplicated, bulleted list of these generalisable narrative event 
        types observed across *all* stories in the batch.
    - Each event type should be concise (e.g., "Introduction of protagonist," "Encounter with antagonist").
    - Do not add any introductory or concluding remarks. Just the bulleted list.
    - If no distinct generalisable event types are found, state "No distinct narrative event types found."
    
    These events should be applicable to stories for pupils of age 6.

BEGIN STORIES FOR ANALYSIS:
{stories_content}
END STORIES FOR ANALYSIS.
"""
    # Calculates tokens in the prompt.
    overhead_prompt_tokens = simple_token_estimate(
        instruction_prompt_template.replace("{stories_content}", "")
    )

    # Calculates number of tokens available for passing input stories to the model.
    MAX_TOKENS_FOR_STORIES_CONTENT = max_context_tokens - overhead_prompt_tokens - max_generation_tokens_per_batch

    # If there are not enough tokens left then raises an error.
    if MAX_TOKENS_FOR_STORIES_CONTENT <= 0:
        print(f"Error: Max context tokens ({max_context_tokens}) too small for prompt overhead ({overhead_prompt_tokens}) and generation buffer ({max_generation_tokens_per_batch}).")
        print("Adjust MAX_CONTEXT_TOKENS or MAX_GENERATION_TOKENS_PER_BATCH, or simplify prompt.")
        return

    current_batch_stories = []
    current_batch_content_tokens = 0
    story_counter = 0

    # Loops through the stories 1 by 1.
    while story_counter < len(stories):
        story = stories[story_counter]
        story_tokens = simple_token_estimate(story)

        # If tokens in the current batch with the tokens for the current story are within the limits or is the first story and
        # its tokens are outside the limits, adds the story to the processing batch.
        if (current_batch_content_tokens + story_tokens <= MAX_TOKENS_FOR_STORIES_CONTENT) or \
           (not current_batch_stories and story_tokens > MAX_TOKENS_FOR_STORIES_CONTENT):
            current_batch_stories.append(story)
            current_batch_content_tokens += story_tokens
            story_counter += 1
        
        else: 
            # If the batch tokens reach the limit before all stories are in the batch, it calls process batch and hands the batch 
            # to the model to process. Then resets the required variables and continues.
            if current_batch_stories:
                print(f"\nProcessing batch with {len(current_batch_stories)} stories (approx. {current_batch_content_tokens} content tokens)...")
                process_batch(
                    current_batch_stories,
                    all_extracted_event_types,
                    llama_cli_path,
                    model_path,
                    story_delimiter,
                    max_context_tokens,
                    max_generation_tokens_per_batch,
                    instruction_prompt_template
                )
                current_batch_stories = []
                current_batch_content_tokens = 0
            # If the story tokens are outside the limits raises an error and skips to the next story.
            elif story_tokens > MAX_TOKENS_FOR_STORIES_CONTENT:
                print(f"Warning: Story {story_counter + 1} is too large ({story_tokens} tokens) for the context window ({MAX_TOKENS_FOR_STORIES_CONTENT} available). Skipping.")
                story_counter += 1

    # Processing of the final batch. Either the last stories or all of them at once.
    if current_batch_stories:
        print(f"\nProcessing final batch with {len(current_batch_stories)} stories (approx. {current_batch_content_tokens} content tokens)...")
        process_batch(
            current_batch_stories,
            all_extracted_event_types,
            llama_cli_path,
            model_path,
            story_delimiter,
            max_context_tokens,
            max_generation_tokens_per_batch,
            instruction_prompt_template
        )

    # Collects all the extracted generalisable events into a list.
    final_unique_event_types = sorted(list(set(all_extracted_event_types)))
    print(f"\nConsolidating and deduplicating {len(all_extracted_event_types)} extracted narrative event types...")
    print(f"Appending {len(final_unique_event_types)} unique narrative event types to '{output_file}'...")

    # Attempts to write the generalisable events to the output file.
    try:
        with open(output_file, "a", encoding="utf-8") as outfile:
            if final_unique_event_types:
                for event_type in final_unique_event_types:
                    outfile.write(f"- {event_type}\n")
            else:
                outfile.write("No distinct narrative event types found.\n")
        print(f"Successfully appended {len(final_unique_event_types)} unique narrative event types to '{output_file}'.")
    except Exception as e:
        print(f"Error writing to output file: {e}")

# Method for processing a batch of stories using llama-cli.
def process_batch(
    batch_stories: list[str],
    all_extracted_event_types: list[str],
    llama_cli_path: str,
    model_path: str,
    story_delimiter: str,
    max_context_tokens: int,
    max_generation_tokens_per_batch: int,
    instruction_prompt_template: str
):
    stories_content_for_prompt = story_delimiter.join(batch_stories)
    
    full_prompt_content = instruction_prompt_template.format(stories_content=stories_content_for_prompt)

    # Specifies the format of the prompt so that the model is passed things in an order it understands and knows what to process
    # and what its ouput is.
    llama_cli_prompt_input_string = (
        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
        f"{full_prompt_content}"
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    )

    # Specifies what variables need to be passed to what part of the model.
    command = [
        llama_cli_path,
        "-m", model_path,
        "-c", str(max_context_tokens),
        "-n", str(max_generation_tokens_per_batch),
        # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
        # probable next token.
        "--temp", "0.7",
        # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
        # words.
        "--top-p", "0.9",
        # Random number generator seed. If same one with same prompt and parameters should always get the same out.
        "--seed", "1",
        "--prompt", llama_cli_prompt_input_string
    ]

    # print("\n--- DEBUG INFO: LLAMA-CLI COMMAND ---")
    # print(f"Command array (truncated prompt): {command[:-1] + [command[-1][:200] + '...'] if len(command[-1]) > 200 else [command[-1]]}")
    # print(f"Full prompt being sent to llama-cli via --prompt argument ({len(llama_cli_prompt_input_string)} chars), truncated to first 500 chars:\n{llama_cli_prompt_input_string[:500]}...")
    # print("--- END DEBUG INFO ---\n")

    try:
        # Runs the specified command for the model and returns what the model outputs to the standard output.
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=True,
            encoding="utf-8"
        )
        raw_output = result.stdout

        # Marks the end of the input to the model.
        user_prompt_end_marker_in_output = "END STORIES FOR ANALYSIS.\n"
        # Marks the start of the model's output.
        assistant_turn_start_marker_in_output = "assistant\n\n" 

        # Finds the last time where the assistant\n\n string is put into the output.
        response_start_marker_index = raw_output.rfind(assistant_turn_start_marker_in_output)

        clean_output = ""
        # If found assistant in the text, returns everything that comes after it in the input.
        if response_start_marker_index != -1:
            clean_output = raw_output[response_start_marker_index + len(assistant_turn_start_marker_in_output):].strip()
            # Remove the <|eot_id|> string if printed in the output.
            clean_output = clean_output.replace("<|eot_id|>", "").strip()
            # Remove the [end of text] string if printed in the output.
            clean_output = clean_output.replace("[end of text]", "").strip()
            
        # If not found, has to process differently.
        else:
            print("Warning: Specific 'assistant\\n\\n' marker not found in raw output. Fallback to parsing after full sent prompt, if found.")
            # Seatches for the full prompt passed to the model and extracts everything after the prompt.
            full_prompt_echo_end_idx = raw_output.rfind(llama_cli_prompt_input_string)
            if full_prompt_echo_end_idx != -1:
                clean_output = raw_output[full_prompt_echo_end_idx + len(llama_cli_prompt_input_string):].strip()
                clean_output = clean_output.replace("<|eot_id|>", "").replace("[end of text]", "").strip()
            else:
                print("Critical Warning: Neither 'assistant\\n\\n' nor full sent prompt found. Processing raw output directly (may contain prompt and garbage).")
                clean_output = raw_output.replace("<|eot_id|>", "").replace("[end of text]", "").strip()

        extracted_events = []
        clean_output_lines = [line.strip() for line in clean_output.splitlines() if line.strip()]

        for line in clean_output_lines:
            # Searches for each bullet point of an event.
            match = re.match(r"^[*-â€¢]\s*(.+)", line)
            # If finds a bullet, extracts the event without the bullet and adds it to the events list.
            if match:
                event_text = match.group(1).strip()
                if event_text != "No distinct narrative event types found.":
                    extracted_events.append(event_text)
            # If finds no events, breaks the loop.
            elif "No distinct narrative event types found." in line:
                extracted_events.append("No distinct narrative event types found.")
                break 
            # Ignores everything else and continues.
            else:
                pass
        
        if not extracted_events and "No distinct narrative event types found." in clean_output:
            extracted_events.append("No distinct narrative event types found.")

        extracted_events = [event.strip() for event in extracted_events if event.strip()]
        all_extracted_event_types.extend(extracted_events)

    except subprocess.CalledProcessError as e:
        print(f"    ERROR: llama-cli exited with non-zero status code {e.returncode}")
        print(f"    Stdout (if any from llama-cli before error): {e.stdout}")
        print(f"    Stderr (llama-cli error messages): {e.stderr}")
    except Exception as e:
        print(f"    General Python error during batch processing: {e}")

# Main method for calling all the above by calling extract_narrative_event_types.
if __name__ == "__main__":

    extract_narrative_event_types(
        input_file=INPUT_FILE_PATH,
        output_file=OUTPUT_FILE_PATH,
        llama_cli_path=LLAMA_CLI_PATH,
        model_path=MODEL_PATH,
        story_delimiter=STORY_DELIMITER,
        max_context_tokens=MAX_CONTEXT_TOKENS,
        max_generation_tokens_per_batch=MAX_GENERATION_TOKENS_PER_BATCH
    )

    # print(f"\nProcessing complete. Check '{OUTPUT_FILE_PATH}' for the consolidated list.")