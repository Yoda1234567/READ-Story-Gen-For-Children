# Authors: Code generated by Google Gemini and ChatGPT online and comments done by Daniel Holgate
# Date: 15/09/2025
# Description: Code for prompting LLaMA to select a source story to match with an initial paragraph with outline and theme 
# to use in the auto-regressive story generation. Version for larger scale, no phoneme generation. 

import subprocess
import re
import os

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
# Must change them for your setup.
LLAMA_CLI_PATH = ""
MODEL_PATH = ""

# Files for input and output. Need to be adjusted for the files you are using. 
# The story file is processed source story dataset, the t_para file is the output file from the extaction of paragraph transitions,
# i_para file is the output file from initial paragraph generation and the output file can be what you want.
STORY_FILE = ""
T_PARA_FILE = ""
I_PARA_FILE = ""
OUTPUT_FILE = ""

# Method for reading in the paragraphs generated in initial paragraph generation and extracting only the actual 
# paragraph text and the story number.
def parse_paragraph_outputs(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    paragraphs = []
    story_num = None

    for line in lines:
        line_strip = line.strip()
        # Searches for the story header with story number and extracts it. Searches for an exact match to the header line
        # and if finds it then returns just the story number in the heading.
        m_outline = re.match(r"--- Story Outline #(\d+) ---", line_strip)
        if m_outline:
            story_num = int(m_outline.group(1))
            continue
        
        # Searches for the event header line and knows that from there until next header is the paragraph.
        m_paragraph = re.match(r"--- Paragraph Output for Event: '.+' ---", line_strip)
        if m_paragraph and story_num is not None:
            paragraph_lines = []
            idx = lines.index(line) + 1
            while idx < len(lines) and not lines[idx].strip().startswith("---"):
                paragraph_lines.append(lines[idx].rstrip('\n'))
                idx += 1
            
            # Joins together all the lines that make up the paragraph.
            paragraph_text = " ".join(paragraph_lines).strip()
            if paragraph_text:
                paragraphs.append((story_num, paragraph_text))

    return paragraphs

# Method for reading in all the source dataset stories to check through.
def load_stories(filename):
    if not os.path.exists(filename):
        print(f"Story file '{filename}' not found.")
        return []
    with open(filename, "r", encoding="utf-8") as f:
        stories = [line.strip() for line in f if line.strip()]
    return stories


# Method for generating a compatability/match score between an initial paragraph and a source story text.
def ask_compatibility_score(initial_paragraph, story_text):
    # Provides the prompt to the model for instructing it in how to go about scoring the source story. 
    prompt = f"""
    
    I want to match a single paragraph to a full children's story that could be applied to the paragraph's context.
    
    Compare the following two passages:
    Passage 1 (Initial Paragraph):
    \"\"\"{initial_paragraph}\"\"\"

    Passage 2 (Story):
    \"\"\"{story_text}\"\"\"

    and give a match score on a scale from 0 to 1, where:
    - 0 means completely unrelated
    - 1 means the stories have the same theme and very similar events
    
    Output ONLY the compatibility score as a decimal number (e.g., 0.75). 
    No extra text. Add 0.1 to the score for each similar event and add 0.3 for a similar theme.
    """

    full_prompt = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n" + prompt + "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

    # Specifies what variables need to be passed to what part of the model.
    command = [
        LLAMA_CLI_PATH,
        "-m", MODEL_PATH,
        "-c", "2048",
        "-n", "50",
        # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
        # probable next token.
        "--temp", "0.7",
        # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
        # words.
        "--top-p", "0.9",
        # Random number generator seed. If same one with same prompt and parameters should always get the same out.
        "--seed", "42",
        "--prompt", full_prompt
    ]

    try:
        # Runs the specified command for the model and returns what the model outputs to the standard output.
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        output = result.stdout

        assistant_marker = "assistant\n\n"
        # Extracts the model's output only.
        start = output.rfind(assistant_marker)
        if start != -1:
            response = output[start + len(assistant_marker):].strip()
        else:
            response = output.strip()

        response = response.replace("<|eot_id|>", "").replace("[end of text]", "").strip()

        # Searches for the score given by the model to the provided source story.
        match = re.search(r"0(\.\d+)?|1(\.0+)?", response)
        if match:
            score = float(match.group())
            return score
        else:
            print(f"Warning: Could not parse a score from model output:\n{response}")
            return 0.0

    except subprocess.CalledProcessError as e:
        print("Error running llama-cli:", e)
        return 0.0

# The main function for selecting a source story that matches the provided initial paragraph. 
# Takes in an initial paragraph, the path to the source story dataset and the desired match score.
# Here the threshold is higher than for the single match, phoneme generation side as was run on the UCT HPC
# and there was time and processing power to have 0.8 as a threshold and a greater possibility of ending
# up scoring all the source stories.
def find_compatible_story_with_threshold(initial_paragraph, story_file_path, threshold=0.8):
    stories = load_stories(story_file_path)
    if not stories:
        print("No stories loaded.")
        return None, 0.0, None

    best_score = -1.0
    best_story = None
    story_num = None

    # Goes through each source story and generates a match score for it. If this is at or above the threshold the story is selected
    # and the process stops. If none are found, after all stories have been scored, the one with the highest score is selected.
    for idx, story in enumerate(stories):
        print(f"Scoring story #{idx+1}/{len(stories)}...")
        score = ask_compatibility_score(initial_paragraph, story)
        print(f"Score: {score}")

        if score > best_score:
            best_score = score
            best_story = story
            story_num = idx + 1

        if score >= threshold:
            print(f"Score {score} exceeds threshold {threshold}. Selecting this story early.")
            return best_story, best_score, story_num

    print(f"No story exceeded the threshold {threshold}. Best score was {best_score}")
    return best_story, best_score, story_num

# Method for extracting the selected source story from transitions file to get the story separated into paragraphs and
# with transitions given as this is the form needed for the auto-regressive transfer.
def extract_story(file_path, story_number):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    story_tag = f"--- Story {story_number} ---"
    next_story_tag = f"--- Story {story_number + 1} ---"

    story_lines = []
    inside_story = False

    # Searches through the file for the story number and then saves all the lines that pertain to the desired story.
    for line in lines:
        if line.strip() == story_tag:
            inside_story = True
            continue
        elif line.strip() == next_story_tag and inside_story:
            break
        elif inside_story:
            story_lines.append(line)

    return ''.join(story_lines).strip()

if __name__ == "__main__":
    paragraph_outputs_file = I_PARA_FILE
    story_file = STORY_FILE
    corpus_file = T_PARA_FILE
    threshold = 0.8
    # Performs the scoring on the specified number of outlines. Set this to the number of stories you want to eventually generate.
    NUM_MATCHES = 50

    paragraphs = parse_paragraph_outputs(paragraph_outputs_file)
    selected_paragraphs = []
    for paragraph in paragraphs:
        if len(selected_paragraphs) >= NUM_MATCHES:
            break
        selected_paragraphs.append(paragraph)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as output_file:
        for i, (selected_story_num, initial_paragraph) in enumerate(selected_paragraphs, start=1):
            print(f"\n[{i}/{NUM_MATCHES}] Matching for paragraph from Story #{selected_story_num}:\n{initial_paragraph}\n")

            chosen_story, score, matched_story_num = find_compatible_story_with_threshold(
                initial_paragraph,
                story_file,
                threshold
            )

            if chosen_story:
                print(f"\n--- Selected Story ---\nCompatibility Score: {score:.2f}")
                print(chosen_story)

                full_story_text = extract_story(corpus_file, matched_story_num)
                output_file.write(f"--- Matched Story {matched_story_num} (Score: {score:.2f}) ---\n")
                output_file.write(full_story_text + "\n\n")
            else:
                print("No story matched for this paragraph.")
                output_file.write(f"### Match {i}: Initial Paragraph from Story {selected_story_num}\n")
                output_file.write(initial_paragraph + "\n\n")
                output_file.write("No compatible story found.\n\n")

    print(f"\nAll {NUM_MATCHES} matches processed. Results saved to {OUTPUT_FILE}")

