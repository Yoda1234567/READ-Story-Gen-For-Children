# Authors: Code generated by Google Gemini and ChatGPT online and comments done by Daniel Holgate
# Date: 16/09/2025
# Description: Code for prompting LLaMA to generate initial paragraphs using the first events in outlines. 
# Version for single story, phoneme incorporation generation. 

import re
import os
import subprocess
import random
import json
import Mistakes

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
# Must change them for your setup.
LLAMA_CLI_PATH = ""
MODEL_PATH = ""

MAX_CONTEXT_TOKENS = 8192 # Specifies the total number of tokens that the model can process at once.
# Specifies the limit of tokens that can be generated by the model in response to a prompt.
MAX_GENERATION_TOKENS_PER_BATCH = 500

# Files for input and output. Need to be adjusted for the files you are using. 
# The outline file is the output from the generation of outlines, the theme file is the output file from theme extraction,
# the event mapping file is the output file from event extraction and conversion, the mistakes file is the output file 
# of Mistakes.py containing the five error phonemes to incorporate and the output file can be what you want.
OUTLINE_FILE_PATH = ""
THEME_FILE_PATH = ""
EVENT_MAPPING_FILE = ""
MISTAKES_FILE = ""
OUTPUT_FILE = ""

# Method for estimating the number of tokens in a text. This will vary in different situations so just an estimate based on the 
# common conversion between words and tokens.
def simple_token_estimate(text):
    return len(text.split()) * 1.3

# Method for reading in all the earlier extracted themes from which will select a random one to go with the outline selected for 
# initial paragraph generation.
def load_themes(theme_file: str) -> list[tuple[str, str]]:
    themes_data = []
    try:
        if not os.path.exists(theme_file):
            print(f"Error: Theme file '{theme_file}' not found.")
            return []

        with open(theme_file, "r", encoding="utf-8") as f:
            content = f.read()

        raw_blocks = re.split(r'---\s*\n', content)
        theme_word_pattern = re.compile(r"Theme Word:\s*(\w+)", re.IGNORECASE)
        theme_sentence_pattern = re.compile(r"Theme Sentence:\s*(.+)", re.IGNORECASE)

        # Goes through each theme block (pair of theme word and theme sentence) and keeps only the correctly formatted pairs.
        for block_content in raw_blocks:
            if not block_content.strip():
                continue
            word_match = theme_word_pattern.search(block_content)
            sentence_match = theme_sentence_pattern.search(block_content)

            theme_word = word_match.group(1).strip() if word_match else None
            theme_sentence = sentence_match.group(1).strip() if sentence_match else None

            if theme_word and theme_sentence:
                themes_data.append((theme_word, theme_sentence))
            else:
                if block_content.strip():
                    print(f"Warning: Incomplete theme block found:\n{block_content.strip()}\nMissing 'Theme Word:' or 'Theme Sentence:'.")

        if not themes_data:
            print(f"Warning: No complete 'Theme Word:' and 'Theme Sentence:' pairs found in '{theme_file}' using the expected format.")
        return themes_data

    except Exception as e:
        print(f"Error reading theme file or parsing themes: {e}")
        return []

# Method for reading all the event-description mapping pairs for mapping the first event in the selected outline to its incorporation
# description for generating the initial paragraph.
def load_event_mappings(mapping_file: str) -> dict[str, str]:
    mappings = {}
    try:
        if not os.path.exists(mapping_file):
            print(f"Warning: Event mapping file '{mapping_file}' not found.")
            return mappings

        with open(mapping_file, "r", encoding="utf-8") as f:
            # Goes through all the event-description pairs and keeps only the correctly formatted descriptions.
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                try:
                    json_object_string = "{" + line + "}"
                    parsed_dict = json.loads(json_object_string)
                    mappings.update(parsed_dict)
                except json.JSONDecodeError as e:
                    print(f"Warning: Skipping malformed JSON line: {line} (Error: {e})")
        return mappings
    except Exception as e:
        print(f"Error loading event mappings: {e}")
        return {}

# Method for reading all the selected outlines from which a single outline will be selected for the initial paragraph
# needing to be generated.
def load_all_outlines(outline_file: str) -> list[list[str]]:
    all_outlines = []
    try:
        if not os.path.exists(outline_file):
            print(f"Error: Outline file '{outline_file}' not found.")
            return []

        with open(outline_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                events = [e.strip() for e in line.split(',') if e.strip()]
                if events:
                    all_outlines.append(events)
        return all_outlines
    except Exception as e:
        print(f"Error reading outline file: {e}")
        return []
    
# Method for extracting the five phonemes to incorporate into the initial paragraph.
def load_mistakes(phoneme_file: str) -> list[str]:
    phonemes = []
    with open(phoneme_file, "r", encoding="utf-8") as f:
        for line in f:
            phonemes = line.split(" ")
    return phonemes
            

# Prompt for creating a list of obstacles to choose from for when the event of adding an obstacle is present at the first
# event in the outline.
obstacle_prompt_template = """Given the story premise: {premise}
Write a list of five unique obstacles that could block the protagonist's major goal in the story.
Each obstacle should be a short phrase, less than 5 words.
Format as a numbered list.
"""

# The main function for generating initial paragraphs from the first event in a selected outline. 
# Takes in an input file, performs what is requested in the prompt and outputs the initial paragraph along with the theme
# and outline used for generating the paragraph.
def generate_paragraph_core(
    event_prompt: str,
    theme_prompt: str,
    llama_cli_path: str,
    model_path: str,
    max_context_tokens: int,
    max_generation_tokens_per_batch: int,
    raw_event_name: str,
    user_input_text: str
) -> str:

    # Provides the prompt to the model for instructing it in how to generate an initial paragraph. Includes the theme
    # and event that the model must incorporate into the paragraph. Also includes the most messed up phonemes (this is randomly 
    # generated for testing purposes)
    instruction_prompt_template = """
    
    I am wanting to write the first paragraphs of some children's stories to teach reading.
    
    Your task is to generate a single paragraph, exactly 3 sentences long, as part of a children's 
    story for a 6 year old.
    
    The story should have the theme of '{theme}'.
    
    This paragraph must incorporate the event: {event}.
    
    Please incorporate words with the sounds of {phonemes[0]}, {phonemes[1]}, {phonemes[2]}, {phonemes[3]}, {phonemes[4]} 
    where it makes sense to. 
    Spell the words as they are spelt not how they would sound. This is important because these are phonemes you are incorporating.

    The grammar and vocabulary should match that of what a 6 year old could understand.
    
    Generate the paragraph now:
    """
    final_event_instruction = event_prompt

    # If the event the model is incorporating is adding an obstacle towards the main goal, it generates five possible obstacles to add.
    if raw_event_name == "add_obstacle_towards_major_goal":
        obstacle_hint_prompt = obstacle_prompt_template.format(premise=user_input_text)
        print(f"Generating obstacle hints for premise: '{user_input_text}'")

         # Specifies what variables need to be passed to what part of the model.
        command_hint = [
            llama_cli_path,
            "-m", model_path,
            "-c", str(max_context_tokens),
            "-n", "100",
            # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
            # probable next token. 
            "--temp", "0.7",
            # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
            # words.
            "--top-p", "0.9",
            # Random number generator seed. If same one with same prompt and parameters should always get the same out.
            "--seed", str(random.randint(1, 10000)),
            "--prompt", "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
                        f"{obstacle_hint_prompt}"
                        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        ]
        try:
            # Runs the specified command for the model and returns what the model outputs to the standard output.
            result = subprocess.run(command_hint, capture_output=True, text=True, check=True)
            raw_hint_output = result.stdout

            assistant_marker = "assistant\n\n"
            # Searches for the assistant marker to find the beginning of the model's output for obstacle generation.
            start = raw_hint_output.rfind(assistant_marker)
            # Attempts to return just the model's output.
            clean_hint_output = raw_hint_output[start + len(assistant_marker):] if start != -1 else raw_hint_output
            clean_hint_output = clean_hint_output.replace("<|eot_id|>", "").replace("[end of text]", "").strip()

            hints = []
            for line in clean_hint_output.split('\n'):
                if line.strip() and re.match(r'^\d+\.', line):
                    hints.append(re.sub(r'^\d+\.\s*', '', line.strip()))

            joined_hints = ", ".join(hints[:5])
            print(f"Generated Obstacle Hints: {joined_hints}\n")

            # Goes through prompt and if contains obstacle hint it knows it requires the list of possible obstacles to add.
            # Adds the list to the prompt if it is required.
            if "{{obstacle_hint}}" in event_prompt:
                final_event_instruction = event_prompt.replace("{{obstacle_hint}}", f"possible examples include: {joined_hints}")
            else:
                final_event_instruction = f"{event_prompt}. Consider obstacles such as: {joined_hints}"

        except subprocess.CalledProcessError as e:
            print("ERROR generating obstacle hints:", e)
            final_event_instruction = event_prompt

    prompt = instruction_prompt_template.format(event=final_event_instruction, theme=theme_prompt, phonemes = load_mistakes("phonemes.txt"))
    full_prompt = "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n" + prompt + "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"

    print("\nðŸŸ¡ Final Prompt Sent to llama.cpp:\n", full_prompt, "\n---------------------------")

    command = [
        llama_cli_path,
        "-m", model_path,
        "-c", str(max_context_tokens),
        "-n", str(max_generation_tokens_per_batch),
        "--temp", "0.7",
        "--top-p", "0.9",
        "--seed", "1",
        "--prompt", full_prompt
    ]

    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        raw_output = result.stdout
        print("\nðŸŸ¢ Raw Output from llama-cli:\n", raw_output, "\n---------------------------")

        assistant_marker = "assistant\n\n"
        start = raw_output.rfind(assistant_marker)
        # Attempts to return just the initial paragraph generated by the model.
        output_clean = raw_output[start + len(assistant_marker):] if start != -1 else raw_output
        output_clean = output_clean.replace("<|eot_id|>", "").replace("[end of text]", "").strip()

        # Improved paragraph extraction: find first paragraph that looks like story content
        paragraph_candidates = [p.strip() for p in output_clean.split('\n\n') if p.strip()]
        paragraph = ""

        for candidate in paragraph_candidates:
            if re.match(r"^[A-Z].*[.?!]$", candidate):
                paragraph = candidate
                break

        if not paragraph:
            paragraph = "[No valid paragraph found]"

        paragraph = paragraph.replace("[end of text]", "").strip()

        print("\nâœ… Final Cleaned Paragraph:\n", paragraph if paragraph else "[EMPTY RESPONSE]", "\n---------------------------")
        return paragraph

    except subprocess.CalledProcessError as e:
        print("ERROR during paragraph generation:", e)
        return ""

# Method for taking in the single selected outline and generating the initial paragraph containing the first outline event.
def process_single_story_outline(outline_raw_events, story_index, story_premise, selected_theme, event_mappings,
                                 llama_cli_path, model_path, max_context_tokens, max_generation_tokens_per_batch):
    output_filename = OUTPUT_FILE
    print(f"\n--- Generating initial paragraph for Outline #{story_index + 1} to '{output_filename}' ---")
    print(f"Story Premise (from Theme Sentence): '{story_premise}'")
    print(f"Overall Theme: '{selected_theme}'")

    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(f"--- Story Outline #{story_index + 1} ---\n")
        f.write(f"Premise (from Theme Sentence): {story_premise}\n")
        f.write(f"Theme: {selected_theme}\n")
        f.write(f"Outline: {outline_raw_events[0]},")
        f.write(f"{outline_raw_events[1]},")
        f.write(f"{outline_raw_events[2]},")
        f.write(f"{outline_raw_events[3]},")
        f.write(f"{outline_raw_events[4]},")
        f.write(f"{outline_raw_events[5]},")
        f.write(f"{outline_raw_events[6]}\n\n")

    if outline_raw_events:
        raw_event = outline_raw_events[0]
        mapped_event = event_mappings.get(raw_event, f"an event related to: {raw_event.replace('_', ' ')}")
        print(f"\nProcessing Initial Event ('{raw_event}'): {mapped_event}")

        paragraph = generate_paragraph_core(
            event_prompt=mapped_event,
            theme_prompt=selected_theme,
            llama_cli_path=llama_cli_path,
            model_path=model_path,
            max_context_tokens=max_context_tokens,
            max_generation_tokens_per_batch=max_generation_tokens_per_batch,
            raw_event_name=raw_event,
            user_input_text=story_premise
        )

        with open(output_filename, "a", encoding="utf-8") as f:
            f.write(f"--- Paragraph Output for Event: '{raw_event}' ---\n")
            f.write(paragraph if paragraph else "[No output generated]")
            f.write("\n\n")
            
    else:
        print(f"Warning: Outline #{story_index + 1} has no events.")

# --- Main ---
if __name__ == "__main__":
    # Generates the five random mistake phonemes to incorporate and saves them to a file. Method is contained in the Mistakes.py
    # python script.
    Mistakes.get_mistakes("phoneme_vocab_display.json")
    themes = load_themes(THEME_FILE_PATH)
    if not themes:
        print("No themes loaded.")
        exit()

    event_mappings = load_event_mappings(EVENT_MAPPING_FILE)
    outlines = load_all_outlines(OUTLINE_FILE_PATH)

    # Specifies to generate a single initial paragraphs for a single selected outline. Do not change this, need to generate 
    # one run at a time to get a different set of random phonemes to incorporate each time.
    num_stories = 1

    # for i in range(num_stories):
    i = random.randint(0, 249)
    outline = outlines[i]
    theme_word, theme_sentence = themes[i % len(themes)]
    process_single_story_outline(
        outline_raw_events=outline,
        story_index=i,
        story_premise=theme_sentence,
        selected_theme=theme_word,
        event_mappings=event_mappings,
        llama_cli_path=LLAMA_CLI_PATH,
        model_path=MODEL_PATH,
        max_context_tokens=MAX_CONTEXT_TOKENS,
        max_generation_tokens_per_batch=MAX_GENERATION_TOKENS_PER_BATCH
    )

    print("\nAll initial paragraphs processed. Check the 'generated_story_outline_X.txt' files.")
